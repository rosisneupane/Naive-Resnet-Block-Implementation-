{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "from torchvision import datasets\r\n",
    "from torchvision import transforms\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper parameters\r\n",
    "batch_size=100\r\n",
    "epochs=5\r\n",
    "learning_rate=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2.6%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "9.9%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n",
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset= datasets.MNIST(root='./data',train=True,download=True,transform=transforms.ToTensor())\r\n",
    "test_dataset=datasets.MNIST(root='./data',train=False,download=False,transform=transforms.ToTensor())\r\n",
    "\r\n",
    "#create loader\r\n",
    "train_loader=torch.utils.data.DataLoader(dataset=train_dataset,batch_size=batch_size,shuffle=True)\r\n",
    "test_loader=torch.utils.data.DataLoader(dataset=test_dataset,batch_size=batch_size,shuffle=False)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset MNIST\n    Number of datapoints: 60000\n    Root location: ./data\n    Split: Train\n    StandardTransform\nTransform: ToTensor()"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 28, 28])\n",
      "tensor([1, 2, 9, 3, 8, 2, 1, 3, 1, 8, 0, 8, 4, 9, 6, 1, 9, 0, 2, 0, 9, 8, 1, 2,\n",
      "        4, 5, 2, 0, 8, 6, 9, 2, 7, 0, 0, 6, 0, 8, 9, 8, 4, 0, 7, 1, 1, 7, 9, 1,\n",
      "        8, 4, 0, 4, 8, 7, 5, 4, 1, 7, 3, 8, 7, 6, 9, 1, 7, 8, 4, 1, 6, 6, 7, 9,\n",
      "        5, 8, 0, 9, 2, 5, 5, 8, 2, 3, 3, 5, 7, 0, 6, 9, 1, 1, 3, 4, 4, 0, 8, 0,\n",
      "        3, 2, 3, 8])\n"
     ]
    }
   ],
   "source": [
    "for images,labels in train_loader:\r\n",
    "    print(images.shape)\r\n",
    "    print(labels)\r\n",
    "    break\r\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\r\n",
    "    \"\"\"Some Information about Block\"\"\"\r\n",
    "    def __init__(self):\r\n",
    "        super(Block, self).__init__()\r\n",
    "        self.con1=nn.Conv2d(in_channels=1, out_channels=16,bias=False, kernel_size=3, padding=1, stride=1)\r\n",
    "        self.batch1=nn.BatchNorm2d(16)\r\n",
    "        self.relu1=nn.ReLU(inplace=True)\r\n",
    "        self.con2=nn.Conv2d(16, 1,bias=False, kernel_size=3, padding=1, stride=1)\r\n",
    "        self.batch2=nn.BatchNorm2d(1)\r\n",
    "        self.Linear1=nn.Linear(1*28*28,100)\r\n",
    "        self.drop = nn.Dropout(p=0.5, inplace=False)\r\n",
    "        self.Linear2=nn.Linear(100,10)\r\n",
    "        \r\n",
    "        \r\n",
    "\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        residual=x.clone()\r\n",
    "        x=self.con1(x)\r\n",
    "        x=self.batch1(x)\r\n",
    "        x=self.relu1(x)\r\n",
    "        x=self.con2(x)\r\n",
    "        x=self.batch2(x)\r\n",
    "        x+=residual\r\n",
    "        x=self.relu1(x)\r\n",
    "\r\n",
    "        \r\n",
    "        x=x.view(-1,1*28*28)\r\n",
    "        x=self.Linear1(x)\r\n",
    "        x=self.drop(x)\r\n",
    "        x=self.Linear2(x)\r\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss=[]\r\n",
    "training_accuracy=[]\r\n",
    "test_loss=[]\r\n",
    "test_accuracy=[]\r\n",
    "model=Block()\r\n",
    "loss_fn=nn.CrossEntropyLoss()\r\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1/5, training loss=0.39696081570039193,training accuracy=88.103,test_loss=0.2286604313738644,test_accuracy=92.800\n",
      "epoch : 2/5, training loss=0.26303749372561774,training accuracy=92.290,test_loss=0.1843459930177778,test_accuracy=94.300\n",
      "epoch : 3/5, training loss=0.22701700985431672,training accuracy=93.168,test_loss=0.16983365898020567,test_accuracy=94.710\n",
      "epoch : 4/5, training loss=0.20618672652790943,training accuracy=93.793,test_loss=0.1616393835004419,test_accuracy=95.360\n",
      "epoch : 5/5, training loss=0.19448520687098306,training accuracy=94.197,test_loss=0.1564936307677999,test_accuracy=95.350\n"
     ]
    }
   ],
   "source": [
    "#training phase\r\n",
    "\r\n",
    "for epoch in range(epochs):\r\n",
    "    correct=0\r\n",
    "    iterations=0\r\n",
    "    iter_loss=0\r\n",
    "    model=model.train()\r\n",
    "    for i,(images,labels) in enumerate(train_loader):\r\n",
    "        preds=model(images)\r\n",
    "        loss=loss_fn(preds,labels)\r\n",
    "        iter_loss+=loss.item()\r\n",
    "        optimizer.zero_grad()\r\n",
    "        loss.backward()\r\n",
    "        optimizer.step()\r\n",
    "        _,predicted_labels=torch.max(preds,1)\r\n",
    "        correct+=(predicted_labels==labels).sum()\r\n",
    "        iterations+=1\r\n",
    "    train_loss.append(iter_loss/iterations)\r\n",
    "    training_accuracy.append(100* correct/len(train_dataset))\r\n",
    "    #testing phase\r\n",
    "    correct=0\r\n",
    "    iter_loss=0\r\n",
    "    iterations=0\r\n",
    "    model=model.eval()\r\n",
    "    for i,(images,labels) in enumerate(test_loader):\r\n",
    "        preds=model(images)\r\n",
    "        loss=loss_fn(preds,labels)\r\n",
    "        iter_loss+=loss.item()\r\n",
    "        _,predicted_labels=torch.max(preds,1)\r\n",
    "        correct+=(predicted_labels==labels).sum()\r\n",
    "        iterations+=1\r\n",
    "    test_loss.append(iter_loss/iterations)\r\n",
    "    test_accuracy.append(100* correct/len(test_dataset))\r\n",
    "    print('epoch : {}/{}, training loss={},training accuracy={:.3f},test_loss={},test_accuracy={:.3f}'.format(epoch+1,epochs,train_loss[-1],training_accuracy[-1],\r\n",
    "    test_loss[-1],test_accuracy[-1]))\r\n",
    "        \r\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true label is  6\n",
      "predicted label is 6\n"
     ]
    }
   ],
   "source": [
    "img=test_dataset[50][0].resize_((1,1,28,28))\r\n",
    "label=test_dataset[50][1]\r\n",
    "prediction=model(img)\r\n",
    "_,predicted_label=torch.max(prediction,1)\r\n",
    "print(\"true label is \",label)\r\n",
    "print(\"predicted label is\",predicted_label.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit ('venv')",
   "name": "python3100jvsc74a57bd074231b8de3aad56b5e0857854bfad9d33b5cff9edd367524eaca1360b07bd906"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "metadata": {
   "interpreter": {
    "hash": "74231b8de3aad56b5e0857854bfad9d33b5cff9edd367524eaca1360b07bd906"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}